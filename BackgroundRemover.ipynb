{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackgroundRemover.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o52Y6QYvUgAh",
        "colab_type": "text"
      },
      "source": [
        "# Introduction:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihEGlbPLQiJL",
        "colab_type": "text"
      },
      "source": [
        "Background separation is one of the most used features in cameras nowadays. And, of course, they use AI for that. So, today, we will be creating a background remover AI using semantic segmentation.\n",
        "\n",
        "We will be using Tiramisu model architecture ([https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326)) as our model and we will be training that model on Matting human dataset provided by AISegment.com ([https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets](https://www.kaggle.com/laurentmih/aisegmentcom-matting-human-datasets)).\n",
        "\n",
        "Now, let's start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9333AZGqCjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfMcGAGRRedC",
        "colab_type": "text"
      },
      "source": [
        "Import all the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeGoWPRK52up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USTwX_zpqgUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk4jjJhHvlye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWS1Fwrcq3-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZEc0KvgRimg",
        "colab_type": "text"
      },
      "source": [
        "Creating Tiramisu model. You could see the image below to better understand what's going on in the code. ![alt text](https://d3i71xaburhd42.cloudfront.net/1d9df46f672b1e22b6f210343be8684f88c0ccca/1-Figure1-1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvVFYrP5tQ4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating dense block\n",
        "def block(x, num_layers):\n",
        "  for i in range(num_layers):\n",
        "    t = x\n",
        "    x = layers.BatchNormalization(axis=1, beta_regularizer=tf.keras.regularizers.l2(0.0001), gamma_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.concatenate([x, t])\n",
        "  return x\n",
        "\n",
        "#Creating Transition down \n",
        "def transition_down(x, num_features):\n",
        "    x = layers.BatchNormalization(axis=1, beta_regularizer=tf.keras.regularizers.l2(0.0001), gamma_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.Conv2D(num_features, (1, 1), padding=\"same\", kernel_initializer=\"he_uniform\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.MaxPooling2D((2, 2), strides=2, padding=\"same\")(x)\n",
        "    return x\n",
        "\n",
        "# Creating Transition up\n",
        "def transition_up(x, num_features):\n",
        "  x = layers.Conv2DTranspose(num_features, strides=2, kernel_size=(3, 3), padding=\"same\")(x)\n",
        "  return x\n",
        "\n",
        "#The function to create Tiramisu model.\n",
        "def create_tiramisu(n_outputs,inputs):\n",
        "  n_pool = 5\n",
        "  growth_rate = 16\n",
        "  num_features = 48\n",
        "  layer_per_block = [4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4]\n",
        "  \n",
        "  x = layers.Conv2D(48, (3, 3), padding=\"same\")(inputs)\n",
        "  skip_connections = []\n",
        "  for i in range(n_pool):\n",
        "    x = block(x, layer_per_block[i])\n",
        "    skip_connections.append(x)\n",
        "    num_features += growth_rate * layer_per_block[i]\n",
        "    x = transition_down(x, num_features)\n",
        "\n",
        "  x = block(x, layer_per_block[n_pool])\n",
        "  skip_connections = skip_connections[::-1]\n",
        "\n",
        "  for i in range(n_pool):\n",
        "    num_features = growth_rate * layer_per_block[n_pool + i]\n",
        "    x = transition_up(x, num_features)\n",
        "    x = layers.concatenate([x, skip_connections[i]])\n",
        "    x = block(x, layer_per_block[n_pool+i+1])\n",
        "\n",
        "  x = layers.Conv2D(n_outputs, kernel_size=(1, 1), padding='same', kernel_initializer=\"he_uniform\")(x)\n",
        "  return layers.Activation('softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca2ywqR5SILH",
        "colab_type": "text"
      },
      "source": [
        "Instantiating Tiramisu model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxejUK2buHOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = layers.Input((224, 224, 3)) #Shape of input image\n",
        "outputs = create_tiramisu(2,inputs)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5qkJZ5WSN-o",
        "colab_type": "text"
      },
      "source": [
        "Now, we have to download the dataset from kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx4PgATTrMMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"anjal123\"\n",
        "os.environ['KAGGLE_KEY'] = \"c7ec2b62b94062a469e70393e205a449\"\n",
        "!kaggle datasets download -d laurentmih/aisegmentcom-matting-human-datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p284xKbySTav",
        "colab_type": "text"
      },
      "source": [
        "Extracting the zip file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmTQWPT7vypl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile(\"/content/aisegmentcom-matting-human-datasets.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqIuYoRoorU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "masks = []\n",
        "images = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfatPxA5ScE3",
        "colab_type": "text"
      },
      "source": [
        "Now, let's read the image file, resize it and append it to a list. The data in image segmentation task is always the actual image and the label is it's mask image. So, we will be creating list of actual image in next block and image masks in block after that.\n",
        "\n",
        "Note: We will not be using all the images provided by the dataset due to memory limitation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8LI8mO_vuPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/matting_human_half/matting/1803151818/\"\n",
        "\n",
        "for folder in sorted(os.listdir(path)):\n",
        "  for filename in sorted(os.listdir(os.path.join(path, folder))):\n",
        "    img = plt.imread(os.path.join(path, folder, filename))\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    img = img[:,:,3]\n",
        "    masks.append(np.round(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5R7hH92fM49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/matting_human_half/clip_img/1803151818/\"\n",
        "\n",
        "for folder in sorted(os.listdir(path)):\n",
        "  for filename in sorted(os.listdir(os.path.join(path, folder))):\n",
        "    img = plt.imread(os.path.join(path, folder, filename))\n",
        "    img = tf.image.resize(img, (224, 224))/255\n",
        "    images.append(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCU1bAkQS9ni",
        "colab_type": "text"
      },
      "source": [
        "Creating the dataset in a format accepted by tensorflow for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIFS1rA-ktKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((images, masks))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX89zwSsTEjv",
        "colab_type": "text"
      },
      "source": [
        "Batching the dataset on the batch size of 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ciz-hfcBteru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = train_data.batch(8).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhSSufFkTRFH",
        "colab_type": "text"
      },
      "source": [
        "Now, let's compile the model created above with loss function as sparse categorical cross-entropy and RMSProp as optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHYES-8sqn4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(1e-3, decay=1-0.99995), metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC0CuXfgTa_O",
        "colab_type": "text"
      },
      "source": [
        "Here comes the exciting part. Let's train the model. We will training it on a few epochs as it takes a lot of time to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_83iaahs_xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "his = model.fit(data,epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLDk7g92UZkX",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU2kAgc4TnIQ",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's test the performance of our model. Load an arbitary image and resize it to our requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibUcLaUyQzHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = \"/content/img.jpg\"\n",
        "img = plt.imread(img_path)\n",
        "plt.imshow(img)\n",
        "img = tf.image.resize(img, (224, 224))\n",
        "test = tf.expand_dims(img,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO4r_gynJY6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = \"/content/maks.png\"\n",
        "mask = plt.imread(img_path)\n",
        "mask= tf.image.resize(mask, (224, 224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjlG1iU8TxT3",
        "colab_type": "text"
      },
      "source": [
        "Do predictions from model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g3kvd43Q_St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lWWf5xTT7JH",
        "colab_type": "text"
      },
      "source": [
        "Showing the final image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Epw-H1EgqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = tf.argmax(pred[0], axis=-1)\n",
        "mask = mask.numpy()\n",
        "mask[:,:,3] = temp\n",
        "plt.imshow(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMl9P4D69zFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imsave('output.png',mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jO3SdveUVNE",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b02OPMlT_N2",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the performance of the trained model is average. This is because of training on less dataset and less epochs. Upon training on an environment with large memory, we could achieve even more."
      ]
    }
  ]
}